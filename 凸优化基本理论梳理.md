# 凸优化基本理论梳理

## 理论内容

[凸优化基础（Convex Optimization basics）](https://blog.csdn.net/zbwgycm/article/details/104460708)

### 基本概念

​	凸集

![image-20240306134740007](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240306134740007.png)

锥、凸锥	

![image-20240308154636414](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240308154636414.png)

![image-20240308154649819](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240308154649819.png)

拟凸函数（需满足不等式）

![image-20240308154755220](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240308154755220.png)

![image-20240308154737070](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240308154737070.png)

凸函数

![image-20240306134843479](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240306134843479.png)

凸函数的一阶、二阶特性
$$
f :\mathbb{R}^\mathrm{n}\to\mathbb{R}
$$
![image-20240306141705487](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240306141705487.png)

二阶特性即为：Hessian 矩阵半正定

凸函数的一阶最优化条件（梯度为0点最优）

![image-20240306143415352](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240306143415352.png)

凸优化问题的层次

![image-20240306143739447](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240306143739447.png)

默认采用向量为列向量
$$
{R}^\mathrm{n}表示{n} \times{1}维度向量
$$
**凸函数判定条件**

![image-20240308154916854](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240308154916854.png)

![image-20240308155018266](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240308155018266.png)

**主要采用Hessian矩阵，也就是二阶导数来判定是否为凸函数**

二阶导数>=0，对应原函数导数从负无穷到正无穷一直在变大，也就是凸函数

![image-20240308155036487](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240308155036487.png)

![image-20240308155317786](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240308155317786.png)

凸优化问题

凸优化问题与一般优化问题的 区别在于：即目标函数为凸函数加可行域为凸集

![image-20240308154013057](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240308154013057.png)

线性规划问题LP

![image-20240306145630246](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240306145630246.png)

二次规划问题QP （Q为对称对角矩阵）Non-Linear Programming

![image-20240306150233655](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240306150233655.png)

[优化理论——二次规划问题求解](https://zhuanlan.zhihu.com/p/375762164)

### 约束问题求解方法（对偶，拉格朗日，FJ，KKT）

![图片](http://mmbiz.qpic.cn/mmbiz_png/REGqUlN1rzLzNnbJdiaslrXmrNzQoJ549Mps23ksgWt8l6osibXPxoo3icvicZ8yfHawicyicGgusLjbG0Z68HYsZ6mQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

*正半定（Positive Semi-Definite, PSD）*

*正定（Positive Definite, PD）*

**无条件(unconstraind problem)**

![图片](http://mmbiz.qpic.cn/mmbiz_png/REGqUlN1rzLzNnbJdiaslrXmrNzQoJ549dp7AUxhZp4etPSGan21goZF4XiclQVgHyLDFryia05ge2nTt0QhO1WUw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

**等式限制条件（equality constrained condition）**

把限制条件改变成对拉格朗日函数梯度为零的情况等价于目标函数的梯度（或者导数）Gradient 和限制条件的梯度（Jacobian）在同一直线上（或者成缩放关系）

![图片](http://mmbiz.qpic.cn/mmbiz_png/REGqUlN1rzLzNnbJdiaslrXmrNzQoJ549TAx49QusvLRww681biae5icgUpibYW6s31cBMhFc4scBYJF9eOavq7LCQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

**不等式限制条件（inequality constrained condition）**

![图片](http://mmbiz.qpic.cn/mmbiz_png/REGqUlN1rzLzNnbJdiaslrXmrNzQoJ549jWJIdOJVe1wU0dq4u9P74vuDvkpz5Cuxe4CibU0jiaExpVetkoVeNib4Q/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

**等式和不等式并存的限制条件 （equality and inequality constrained condition）**

![图片](http://mmbiz.qpic.cn/mmbiz_png/REGqUlN1rzLzNnbJdiaslrXmrNzQoJ549QqX6FJmvvV2hVnAzFUFk8OmicGCibbEMtpuTMMiaLiad2CVynDcSyVp5wQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

### 部分优化问题及方法讲解

（1）控制非零解数目

前向选择、后向消去

（2）含绝对值优化

引入额外变量使其转换成易求解的线性规划问题

![image-20240306192858640](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240306192858640.png)

（3）梯度下降法、牛顿法

泰勒展开式形式为

![image-20240306193836663](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240306193836663.png)

梯度下降法

![image-20240306194017016](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240306194017016.png)

**牛顿法（自适应设置学习率)**

![image-20240306194150009](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240306194150009.png)

**牛顿法需要得到目标函数的一阶导数（对应：Jacobian，雅可比矩阵）和二阶导数（对应：Hessian，海森矩阵）**

雅可比矩阵表示为J_F

![image-20240306200922393](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240306200922393.png)

**注意：雅可比矩阵可针对单输出（标量）函数或者向量值函数  而海森矩阵只可用于单输出（标量）函数**

![image-20240307135016562](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240307135016562.png)

![image-20240307134723127](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240307134723127.png)

拟牛顿法采用近似海森矩阵完成实际应用（梯度下降法、牛顿法求解的问题都是单目标优化问题-大多为无约束）

### 对偶函数

**凸优化有一个很重要的性质：局部最优点必定是全局最优点**

**同时：对偶问题一定是凸优化问题**

对偶问题与原问题互为对偶

Boyd大佬在《凸优化》中对该问题的证明原文

![图片名称](https://img-blog.csdnimg.cn/20201014162902524.png)

![image-20240307141149326](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240307141149326.png)

![image-20240307141213301](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240307141213301.png)

当优化模型约束很多，直接求解比较困难，这个时候可以考虑将原问题转化为它的对偶问题，使得约束降低；

或者原模型目标非凸，对偶法可以求出原模型目标函数的下界。

#### 用对偶函数求解问题的一般步骤（对偶函数定义为拉格朗日函数的最小值）

（1）使用**拉格朗日乘子法**将原问题转化为朗格朗日函数优化问题(可变为min-max)---去除约束条件

（2）将朗格朗日函数优化问题转化为对偶问题

（3）求解对偶问题得出原问题最优值的下界

**注意：一般都是特指单目标优化问题**

#### 拉格朗日函数

**等式约束的拉格朗日乘子（可取任意值）与不等式约束的拉格朗日乘子（一定要大于等于>=0）**

**注意：不等式约束一定要转化成右边小于等于0的情况**

![image-20240307143926926](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240307143926926.png)

示例：等式与不等式联合约束的优化模型

![image-20240307144028142](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240307144028142.png)

**构建原问题的等价问题**

![image-20240307154953965](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240307154953965.png)

![image-20240307144049488](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240307144049488.png)

![image-20240307155402679](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240307155402679.png)

**对偶问题**

![image-20240307155444868](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240307155444868.png)

为了得到无约束问题的对偶形式，一般首先在原问题中加入一个虚拟的中间变量，从而引入等式约束。再通过拉格朗日方程得到对偶形式。通常这种变换并不是唯一的，不同的变换可能导致不同的对偶问题。

**对偶问题的构造并不是唯一的**

**构建对偶问题（拉格朗日对偶问题），就是极大化对偶函数的值**

![image-20240307144109484](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240307144109484.png)

**通过构建对偶问题可以获得原问题最优值的下界**

例如，一个简单的生产计划原问题：

在满足生产能力、原料供应等各种约束条件下，如何安排各产品的生产量以使总利润最大。

对偶问题可能变为：

在保证给定的收益水平下，如何分配有限的资源（如生产能力、原料）使得总的资源消耗最小。

**在强对偶条件下，对偶问题与原问题是等价的，在弱对偶条件下对偶问题可以给予原问题一定的参考信息**

![image-20240307144529700](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240307144529700.png)

凸函数加凸约束加存在解==Slater条件

**注意：满足Slater条件就一定是强对偶，但是强对偶不一定满足Slater条件**

![image-20240307145247191](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240307145247191.png)

凸函数构成的不等式约束仍然为凸集，非凸函数则不一定。

![image-20240307145618791](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240307145618791.png)

![image-20240307145636501](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240307145636501.png)

![image-20240307145659882](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240307145659882.png)

![image-20240307145739063](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240307145739063.png)

![image-20240307145756525](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240307145756525.png)

### 拉格朗日乘子法与KKT条件

**拉格朗日乘子法主要应用于求解带约束条件的优化模型**

拉格朗日松弛条件：乘子为0，拉格朗日紧致条件：乘子>0

**KKT条件是强对偶关系的必要条件，即：如果原问题与对偶问题是强对偶关系，则一定有KKT**

如果强对偶则有KKT

补充：强对偶关系的充要条件没有找到，但强对偶关系的必要条件是KKT条件

存在KKT条件时：最优解一定满足KKT，但满足KKT的解不一定是最优解，包含最优解关系

![image-20240307190836898](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240307190836898.png)

λ=0时，g_i(x)<0，在可行域内

λ>0时，g_i(x)=0，在可行域边界处，值为0但梯度不为0（可以从拉格朗日函数梯度为0推出）

这里的λ指的是不等式约束的拉格朗日乘子

![image-20240307191015872](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240307191015872.png)

原问题，对偶问题 + 互补松弛

互补松弛条件：既包含了松弛条件又包含了紧致条件

KKT条件使用

（1）等式约束问题

拉格朗日函数导数为0，等式为0

等式约束为0，可以写成对ν（这里是λ）的**偏导数**为0

![image-20240308135103072](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240308135103072.png)

（2）不等式约束问题

拉格朗日函数导数为0，互补松弛条件为0（不等式约束对应项），λ>=0，不等式<=0

（3）等式与不等式约束问题

拉格朗日函数导数为0，等式为0，互补松弛条件为0（不等式约束对应项），λ>=0，不等式<=0

**为什么λ必须>=0**

拉格朗日函数目的是求出比原问题最优解小的值(在可行域内)，因此要保证λ小于等于0，才能保证求出的值为最优解下界，从而采用对偶问题求出最接近原问题最优解的最大下界(最接近最优值max-min，可以首先确定x的值再调节λ和u参数的值)，如果λ大于等于0则是最优解的上界，则变成极小化最优值的上界min，即在可行域内取最优值的时候，有λ和u都为0，就又变成了原问题，没有意义。（构建对偶问题非常巧妙）

![image-20240308163634358](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240308163634358.png)

对偶问题求解出来的一定是一个在可行域内小于等于最优值的解，差值为对偶间隙

**为什么对偶问题一定是凸优化问题**

对偶问题中，max对应的目标函数中x为常数，可行域为凸集且目标函数变成了参数的线性组合，因此一定为凸优化问题！

![image-20240308135221843](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240308135221843.png)

![image-20240308135233367](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240308135233367.png)

![image-20240308135748914](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240308135748914.png)

![image-20240308135810284](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240308135810284.png)

![image-20240308135821871](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240308135821871.png)

![image-20240308135838780](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240308135838780.png)

### 多目标优化

概述：![image-20240308140205622](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240308140205622.png)

![image-20240308140230831](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240308140230831.png)

![image-20240308140240651](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240308140240651.png)

![image-20240308140249430](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240308140249430.png)

### 内点法

![image-20240308145955988](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240308145955988.png)

#### 障碍函数法

**特点：引入的障碍函数能够在满足定义域时符合互补松弛条件，从而优化原问题**

![image-20240308151416373](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240308151416373.png)

![image-20240308151447253](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240308151447253.png)

![image-20240308151522077](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240308151522077.png)

![image-20240308151536033](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240308151536033.png)

#### 原始对偶内点法

![image-20240308151603457](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240308151603457.png)

![image-20240308151616239](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240308151616239.png)

### 线性规划

![image-20240308140941630](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240308140941630.png)

#### 圆形法

![image-20240308140953707](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240308140953707.png)

#### 内点法

![image-20240308141041569](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240308141041569.png)

![image-20240308152201179](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240308152201179.png)

![image-20240308152211710](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240308152211710.png)

![image-20240308152223286](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240308152223286.png)

#### 解释牛顿迭代法

牛顿迭代法是一种求方程f(x)=0或方程组F(x)=0近似解的一种方法

![image-20240308153302414](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240308153302414.png)

![image-20240308153310713](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240308153310713.png)

**Hessian，海森矩阵，就对应于原函数进行二阶泰勒展开式中的二阶偏导数，二阶导数**

**再次理解牛顿法**

![image-20240308153751699](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240308153751699.png)

![image-20240308153807299](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240308153807299.png)

![image-20240308153828545](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240308153828545.png)

### 病态问题和条件数

在CV领域大部分问题都是非适定问题（ill-posed problem）

![image-20240308160554086](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240308160554086.png)

![image-20240308160601379](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240308160601379.png)

![image-20240308160625683](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240308160625683.png)

![image-20240308160638252](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240308160638252.png)

![image-20240308160649386](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240308160649386.png)

![image-20240308160710327](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240308160710327.png)

![image-20240308160725408](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240308160725408.png)

### lasso问题

**本质是在目标函数中添加L1正则化项**

lasso问题是一个非线性、不可微函数优化问题，求解过程不容易。

Lasso问题在统计学和机器学习中指的是利用Least Absolute Shrinkage and Selection Operator（LASSO）方法来解决模型选择和变量选择的问题。LASSO是一种正则化回归技术，它通过在损失函数中添加一个绝对值惩罚项（L1正则化项），使得在最小化模型预测误差的同时，对模型参数进行压缩（shrinking）。

![image-20240308161725911](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240308161725911.png)

### 近端梯度法

将不可微的目标函数分解为可微函数与不可微函数的组合，借助可微函数进行梯度下降求近似解

可分解的目标函数

![image-20240308175316944](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240308175316944.png)

近端投影

![image-20240308175342616](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240308175342616.png)

近端梯度下降

![image-20240308175402174](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240308175402174.png)

### 随机梯度下降法

当数据规模比较小时，我们可以使用常规的梯度下降方法计算在所有数据上的梯度并进行更新迭代。而当数据规模比较大时，每次计算所有数据梯度的开销将会非常巨大。由于随机梯度下降可以大大减小计算开销，因此常用于大规模数据优化中。

![image-20240308163125517](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240308163125517.png)

![image-20240308163140111](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240308163140111.png)

![image-20240308163210163](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240308163210163.png)

![image-20240308163241880](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240308163241880.png)

![image-20240308163302329](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240308163302329.png)

![image-20240308163313456](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240308163313456.png)

### 对偶梯度上升法和增广拉格朗日法（乘子法）

**共轭函数**

![image-20240308182553156](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240308182553156.png)

![image-20240308182605037](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240308182605037.png)

**对偶梯度法、对偶梯度上升法**

![image-20240308180355398](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240308180355398.png)

![image-20240308180410527](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240308180410527.png)

**对偶分解法**

![image-20240308180439099](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240308180439099.png)

![image-20240308180457450](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240308180457450.png)

![image-20240308180509259](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240308180509259.png)

![image-20240308180521104](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240308180521104.png)

sup函数与inf函数分别对应求一个函数的上确界与下确界

![image-20240308181433265](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240308181433265.png)

**对偶上升法 (Dual Ascent)：对应对偶问题的迭代求解方法**

梯度下降法：使目标函数逐渐达到最小值

梯度上升法：使目标函数逐渐达到最大值

![image-20240308182749482](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240308182749482.png)

![image-20240308182805812](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240308182805812.png)

![image-20240308182825599](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240308182825599.png)

![image-20240308182842247](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240308182842247.png)

将对偶问题的求解，分解为两次迭代，逐渐求出最优值

**增广拉格朗日法（乘子法）**

![image-20240308184245334](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240308184245334.png)

增广拉格朗日法原问题变为无约束问题，并引入以下两项：

![image-20240308184426174](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240308184426174.png)

![image-20240308184558642](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240308184558642.png)

增广拉格朗日形式

![image-20240308184517555](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240308184517555.png)

对偶梯度上升法虽然可以做变量分解，但是需要较强的约束条件保证收敛；而对于乘子法而言，虽然有较好的收敛性，但是却失去了可分解性。

### 交替方向乘子法

交替方向乘子法（Alternating Direction Method of Multipliers，ADMM）有较好的收敛性和分解性

![image-20240308180844872](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240308180844872.png)

ADMM重复迭代，也就是采用对偶梯度上升法 

![image-20240308180929549](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240308180929549.png)

## 一阶与二阶优化

### 一阶优化方法

梯度下降、随机梯度下降、minibatch

将数据集分成几个batches，就不必等到算法遍历整个数据集后才更新权重和偏差，而是在每个所谓的Mini-batch结束时进行更新。这使得我们能够快速将成本函数移至全局最小值，并在每个epoch中多次更新权重和偏差。最常见的Mini-batch大小是16、32、64、128、256和512。

### 二阶优化方法

牛顿法、拟牛顿法

牛顿法比梯度下降法快，但牛顿法要算Hessian矩阵的逆，比较费时间

拟牛顿法通过构造一系列正定对称矩阵来近似Hessian矩阵的逆，从而简化了运算的复杂度

### 一、二阶结合优化方法

（1）共轭梯度法（Conjugate Gradient）

（2）含有动量的梯度下降：类比惯性，在参数更新时，不光考虑当前的梯度方向，还要考虑过去累积梯度的方向，但需要乘以衰减系数（类比摩擦力）可以减小落在局部最小值的概率，但不能避免

![img](https://img-blog.csdnimg.cn/20210414150646300.png)

（3）牛顿加速梯度下降法： Newton accelerate gradient descent: NAG：既考虑动量，又考虑二阶导的变化趋势

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210414150723525.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zNjM3ODUwOA==,size_16,color_FFFFFF,t_70)

（4）Adagrad、RMSProp、Adadelta：根据不同的参数调整梯度更新量。Adadelta不需要设置默认学习率

（5）Adam

![image-20240308191214743](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240308191214743.png)

（6）signSGD

SGD里面，梯度真正有用的是方向而不是大小。所以，即使你只保留梯度的符号来对模型进行更新，也能得到收敛的效果。甚至有些情况下，这么做能减少梯度的噪声，使得收敛速度更快。

signSGD直接把求gradient变成求sign

![在这里插入图片描述](https://img-blog.csdnimg.cn/direct/af51f6a299c043eea41dbf0aaa3f5ddf.png)

（7）分布式训练下的majority vote

![在这里插入图片描述](https://img-blog.csdnimg.cn/direct/9fcbe81943a24f12abbdfbeee669a985.png)

（8）Lion

论文：Symbolic Discovery of Optimization Algorithms
主要内容：自动搜索优化器的
方式：通过数千TPU小时的算力搜索并结合人工干预，得到了一个速度更快、更省显存的优化器Lion，并在图像分类、图文匹配、扩散模型、语言模型预训练和微调等诸多任务上做了充分的实验，多数任务都显示Lion比目前主流的AdamW等优化器有着更好的效果。

![image-20240308192029978](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240308192029978.png)

Lion相比AdamW参数更少（少了个ϵ），少缓存了一组参数v（所以更省显存），并且去掉了AdamW更新过程中计算量最大的除法和开根号运算（所以更快）。

![在这里插入图片描述](https://img-blog.csdnimg.cn/direct/c0a399d42ec44f5b9b3ae527dd56cb37.png)

跟Lion一样，SIGNUM也用到了符号函数处理更新量，而且比Lion更加简化（等价于Lion在β1=β2和λt=0的特例），但是很遗憾，SIGNUM并没有取得更好的效果，它的设计初衷只是降低分布式计算中的传输成本。Lion的更新规则有所不同，尤其是动量的更新放在了变量的更新之后，并且在充分的实验中显示出了它在效果上的优势。

## 零阶优化

![image-20240308192402945](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240308192402945.png)

![image-20240308192424340](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240308192424340.png)

## 多层优化

![image-20240308192536011](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240308192536011.png)

![image-20240308192656760](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240308192656760.png)

![image-20240308192723121](C:\Users\93288\AppData\Roaming\Typora\typora-user-images\image-20240308192723121.png)

神经网络在梯度更新的时候是如何解决多层参数优化问题的？

神经网络的多层优化与优化问题中的多层优化不同，神经网络可以使用简单的反向传播算法更新参数，但优化问题中没办法像这样去更新参数。
